# === HUGGINGFACE SETTINGS CONFIGURATION ===
# You can configure all aspects of your huggingface 
hf_configuration:
  token: $HF_TOKEN # you can get one from here: https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication
  hf_organization: $HF_ORGANIZATION # set this env variable to be your username!
  private: false # true by default, set to false to make the dataset publicly viewable!
  hf_dataset_name: yourbench_example # name of the dataset to save the traces, and generated questions to.
  concat_if_exist: false # concatenate the dataset if it already exists

# you can also set this to be a local dataset
# local_dataset_dir: /some/path

# === MODEL CONFIGURATION ===
model_list: 
  # to use the default huggingface inference
  - model_name: Qwen/Qwen2.5-VL-72B-Instruct
    provider:  hf-inference # as a default value
    api_key: $HF_TOKEN
    max_concurrent_requests: 32
  # to use the default huggingface inference
  - model_name: Qwen/Qwen2.5-72B-Instruct
    provider: novita # or another desired provider, the list of available providers is here: https://huggingface.co/docs/huggingface_hub/guides/inference#supported-providers-and-tasks
    api_key: $HF_TOKEN
    max_concurrent_requests: 32

model_roles:
  ingestion:
    - Qwen/Qwen2.5-VL-72B-Instruct # you should use a vision supported model for ingestion
  summarization:
    - Qwen/Qwen2.5-72B-Instruct
  chunking:
    - intfloat/multilingual-e5-large-instruct # your sentence level chunking model
  single_shot_question_generation:
    - Qwen/Qwen2.5-72B-Instruct
  multi_hop_question_generation:
    - Qwen/Qwen2.5-72B-Instruct

pipeline:
  # to convert your documents from their source format to markdown
  ingestion:
    run: true
    # set this to where your raw documents are located
    source_documents_dir: example/data/raw
    # .... and this to where you want them to be processed to
    output_dir: example/data/processed

  # to convert your documents to a huggingface dataset
  upload_ingest_to_hub:
    run: true
    source_documents_dir: example/data/processed

  # to create a global summary of your documents
  summarization:
    run: true
    max_tokens: 4096       # max tokens per chunk
    token_overlap: 100     # (optional) overlap between chunks
    encoding_name: cl100k_base  # "cl100k_base" for Qwen/GPT, "gpt2" for others
  
  chunking:
    run: true
    chunking_configuration:
      chunking_mode: fast_chunking   # now uses token-based chunking via tiktoken
      l_max_tokens: 128              # controls chunk size
      token_overlap: 0               # optional: token overlap for continuity
      encoding_name: cl100k_base     # "cl100k_base" for GPT/Qwen/DeepSeek/ChatML, "gpt2" for others

      # The following are only used for semantic_chunking
      l_min_tokens: 64
      tau_threshold: 0.8

      # Multi-hop chunking settings (used in both modes)
      h_min: 2
      h_max: 3
      num_multihops_factor: 3
  
  single_shot_question_generation:
    run: true
    # you can add any additional instructions you want here! try it out!
    additional_instructions: "Generate questions to test a curious adult"
    # for cost reduction. if you set all, then all chunks will be used
    chunk_sampling:
      mode: "count" # or "all" for all chunks
      value: 5
      random_seed: 123
  
  multi_hop_question_generation:
    run: true
    additional_instructions: "Generate questions to test a curious adult"
    # for cost reduction
    chunk_sampling:
      mode: "percentage" # or "count" for a fixed number
      value: 0.3
      random_seed: 42

  # this combines your single shot and multi-hop questions into one nice dataset!
  lighteval:
    run: true
