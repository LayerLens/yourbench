# === ADVANCED CONFIGURATION FILE ===
# This configuration file retains the same functionality as the simple example, but shows you how to use more advanced features.

# === HUGGINGFACE SETTINGS CONFIGURATION ===
hf_configuration:
  token: $HF_TOKEN # you can get one from here: https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication
  hf_organization: $HF_ORGANIZATION # (you can set this your huggingface account, or an organization you are a member of. if not, it'll use your HF username. )
  private: false # set to true if you want to make the dataset private. this is true by default for privacy reasons.
  hf_dataset_name: yourbench_advanced_example # name of the dataset to save the traces, and generated questions to.
  concat_if_exist: false # concatenate the dataset if it already exists. this will help you repeatedly run the pipeline without overwriting the existing dataset and adding new traces / questions.
  # local_dataset_dir: /some/path # path to save the dataset to. if this is not provided, the dataset will be saved to the current working directory
  # local_saving: false # if true, the dataset will be saved to the local directory specified in local_dataset_dir. if false, the dataset will be saved to the huggingface hub.

# === MODEL CONFIGURATION ===
# you can define a list of models here, to use for each stage of the pipeline, based on your needs.
model_list: 
  # to use a default huggingface inference provider
  - model_name: Qwen/Qwen3-30B-A3B
    provider: fireworks-ai
  
  # to use a self-hosted vLLM server - you can use any openai compatible server here
  - model_name: Qwen/Qwen3-4B
    base_url: http://localhost:8000/v1
    api_key: $VLLM_API_KEY
    max_concurrent_requests: 128 # if you want to process large batches in parallel
  
  # to use an openai model
  - model_name: gpt-4.1
    base_url: https://api.openai.com/v1
    api_key: $OPENAI_API_KEY
    max_concurrent_requests: 8 # if you have a low rate limit, you can lower this number
  
  # to use a gemini model
  - model_name: gemini-2.5-flash-preview
    base_url: https://generativelanguage.googleapis.com/v1beta/openai/
    api_key: $GEMINI_API_KEY

# === MODEL ROLES CONFIGURATION ===
model_roles:
  ingestion:
    - Qwen/Qwen3-30B-A3B # you should likely use a vision supported model for ingestion
  summarization:
    - Qwen/Qwen3-30B-A3B
    # - gemini-2.5-flash-preview # as an example, for long context windows
  chunking:
    - intfloat/multilingual-e5-large-instruct # your sentence level chunking model
  single_shot_question_generation:
    - Qwen/Qwen3-30B-A3B
    # - gpt-4.1 # as an example, to generate questions with multiple models.
    # - gemini-2.5-flash-preview # as an example, for long context windows
  multi_hop_question_generation:
    - Qwen/Qwen3-30B-A3B
    # - gpt-4.1 # as an example, to generate questions with multiple models.
    # - gemini-2.5-flash-preview # as an example, for long context windows

pipeline:
  # to convert your documents from their source format to markdown
  ingestion:
    run: true
    # set this to where your raw documents are located
    source_documents_dir: example/data/raw
    # .... and this to where you want them to be processed to
    output_dir: example/data/processed

  # to convert your documents to a huggingface dataset
  upload_ingest_to_hub:
    run: true
    # set this to where your processed documents are located 
    source_documents_dir: example/data/processed

  # to create a global summary of your documents
  summarization:
    run: true
    max_tokens: 16384       # max tokens per chunk.
    # max_tokens: 128000     # if your model has a very large context window
    token_overlap: 128     # (optional) overlap between chunks
    # token_overlap: 1024     # to preserve more continuity and context at the expense of chunk size
    encoding_name: cl100k_base  # "cl100k_base" for Qwen/GPT, "gpt2" for others
  
  chunking:
    run: true
    chunking_configuration:
      chunking_mode: fast_chunking   # now uses token-based chunking via tiktoken
      # chunking_mode: semantic_chunking # to use semantic chunking
      l_max_tokens: 128              # controls chunk size
      # l_max_tokens: 1024          # to get more detailed and abstract questions.
      token_overlap: 0               # optional: token overlap for continuity
      # token_overlap: 128             # to preserve more continuity and context at the expense of chunk size
      encoding_name: cl100k_base     # "cl100k_base" for GPT/Qwen/DeepSeek/ChatML, "gpt2" for others

      # The following are only used for semantic_chunking
      # l_min_tokens: 64
      # tau_threshold: 0.8

      # Multi-hop chunking settings
      # h_min: 2 # minimum number of chunks per multi hop chunk configuration
      # h_max: 5 # maximum number of chunks per multi hop chunk configuration
      # num_multihops_factor: 5 # increasing this number will increase the overall number of chunk combinations
  
  single_shot_question_generation:
    run: true
    # you can add any additional instructions you want here! try it out!
    # additional_instructions: "Generate non obvious factual questions that tests a college student's knowledge. Make sure the questions have concrete answers."
    # for cost reduction. if you set all, then all chunks will be used
    # chunk_sampling:
    #   mode: "count" # or "all" for all chunks
    #   value: 5
    #   random_seed: 123
  
  multi_hop_question_generation:
    run: true
    # additional_instructions: "Generate longer, reasoning based questions to test abstract reasoning of a PhD student."
    # for cost reduction
    # chunk_sampling:
    #   mode: "percentage" # or "count" for a fixed number
    #   value: 0.3
    #   random_seed: 42

  # this combines your single shot and multi-hop questions into one nice dataset!
  lighteval:
    run: true

  citation_score_filtering:
    run: true

